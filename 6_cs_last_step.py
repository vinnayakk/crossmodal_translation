# -*- coding: utf-8 -*-
"""6_CS_last_step.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ep3LZOY6WPqFRLg0tA5rl8tCdAbpPA9t
"""

from google.colab import drive
drive.mount('/content/drive')

# Required Libraries
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import tensorflow.keras.backend as K

# Load the latent representations from Beta VAE model
latent_video_train = np.load("/content/drive/My Drive/symbol_grounding_prototype/latents_CS/latent_video_train_object2.npy")
latent_audio_train = np.load("/content/drive/My Drive/symbol_grounding_prototype/latents_CS/latent_audio_train_object2.npy")

latent_video_test = np.load("/content/drive/My Drive/symbol_grounding_prototype/latents_CS/latent_video_test_object2.npy")
latent_audio_test = np.load("/content/drive/My Drive/symbol_grounding_prototype/latents_CS/latent_audio_test_object2.npy")

# Data preparation
video_train, video_val, audio_train, audio_val = train_test_split(latent_video_train, latent_audio_train, test_size=0.2, random_state=42)

# Convert to float32 for TensorFlow compatibility
video_train = video_train.astype(np.float32)
audio_train = audio_train.astype(np.float32)
video_val = video_val.astype(np.float32)
audio_val = audio_val.astype(np.float32)
video_test = latent_video_test.astype(np.float32)
audio_test = latent_audio_test.astype(np.float32)

# Reshape for sequence (adding timestep dimension)
def reshape_data(data):
    return data.reshape(data.shape[0], 1, data.shape[1])

video_train = video_train.reshape(video_train.shape[0], 1, video_train.shape[1])
audio_train = audio_train.reshape(audio_train.shape[0], 1, audio_train.shape[1])
video_val = video_val.reshape(video_val.shape[0], 1, video_val.shape[1])
audio_val = audio_val.reshape(audio_val.shape[0], 1, audio_val.shape[1])
video_test = video_test.reshape(video_test.shape[0], 1, video_test.shape[1])
audio_test = audio_test.reshape(audio_test.shape[0], 1, audio_test.shape[1])

print("Shapes after reshaping:")
print(f"Video Train: {video_train.shape}, Audio Train: {audio_train.shape}")
print(f"Video Val: {video_val.shape}, Audio Val: {audio_val.shape}")
print(f"Video Test: {video_test.shape}, Audio Test: {audio_test.shape}")

# Prepare decoder inputs (zero-initialized) and targets
decoder_inputs_train = np.zeros_like(audio_train)
decoder_inputs_val = np.zeros_like(audio_val)
decoder_inputs_test = np.zeros_like(audio_test)

decoder_inputs_train[:, :-1, :] = audio_train[:, 1:, :]
decoder_inputs_val[:, :-1, :] = audio_val[:, 1:, :]
decoder_inputs_test[:, :-1, :] = audio_test[:, 1:, :]

# Model architecture
latent_dim = 8
input_dim = video_train.shape[2]
output_dim = audio_train.shape[2]

# Encoder
encoder_inputs = keras.Input(shape=(1, input_dim))
encoder_lstm = keras.layers.LSTM(latent_dim, return_state=True)
_, state_h, state_c = encoder_lstm(encoder_inputs)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = keras.Input(shape=(1, output_dim))
decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = keras.layers.Dense(output_dim, activation='linear')
decoder_outputs = decoder_dense(decoder_outputs)

# Custom metric: Cosine Similarity
def cosine_similarity(y_true, y_pred):
    y_true = K.l2_normalize(y_true, axis=-1)
    y_pred = K.l2_normalize(y_pred, axis=-1)
    return K.mean(K.sum(y_true * y_pred, axis=-1))

# Compile model
model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['mae', cosine_similarity])
model.summary()

decoder_target_train = np.copy(audio_train)
decoder_target_val = np.copy(audio_val)

# Model training
history = model.fit([video_train, decoder_inputs_train], decoder_target_train, batch_size=64, epochs=25, validation_data=([video_val, decoder_inputs_val], decoder_target_val))

# Evaluation metrics
def print_metrics(history):
    print("\n=== Training Metrics ===")
    print(f"Final Train Loss: {history.history['loss'][-1]:.4f}")
    print(f"Final Train MAE: {history.history['mae'][-1]:.4f}")
    print(f"Final Train Cosine: {history.history['cosine_similarity'][-1]:.4f}")

    print("\n=== Validation Metrics ===")
    print(f"Final Val Loss: {history.history['val_loss'][-1]:.4f}")
    print(f"Final Val MAE: {history.history['val_mae'][-1]:.4f}")
    print(f"Final Val Cosine: {history.history['val_cosine_similarity'][-1]:.4f}")

print_metrics(history)

# Test evaluation
decoder_target_test = np.copy(audio_test)
test_results = model.evaluate(
    [video_test, decoder_inputs_test],
    decoder_target_test,
    verbose=0
)
print("\n=== Test Metrics ===")
print(f"Test Loss: {test_results[0]:.4f}")
print(f"Test MAE: {test_results[1]:.4f}")
print(f"Test Cosine: {test_results[2]:.4f}")

# Visualization
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss Evolution')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['cosine_similarity'], label='Train Cosine')
plt.plot(history.history['val_cosine_similarity'], label='Val Cosine')
plt.title('Cosine Similarity Evolution')
plt.legend()
plt.show()

# Save model
model.save("/content/drive/My Drive/symbol_grounding_prototype/models_CS/DST_Seq2Seq_Model.h5")

# Inference models
encoder_model = keras.Model(encoder_inputs, encoder_states)

# Decoder inputs for inference
decoder_state_input_h = keras.Input(shape=(latent_dim,))
decoder_state_input_c = keras.Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]


decoder_input_single = keras.Input(shape=(1, output_dim))

# Decoder LSTM (uses input + states from encoder)
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_input_single, initial_state=decoder_states_inputs
)
decoder_states_outputs = [state_h, state_c]

# Final output layer (dense layer from training)
decoder_outputs = decoder_dense(decoder_outputs)

# Create the inference-time decoder model
decoder_model = keras.Model(
    [decoder_input_single] + decoder_states_inputs,
    [decoder_outputs] + decoder_states_outputs
)


# Prediction function

def predict_audio_batch(video_sequences, max_length=1):
    batch_size = video_sequences.shape[0]

    # Get encoder hidden states
    states = encoder_model.predict(video_sequences, verbose=0)

    # Start with zero input
    target_seq = np.zeros((batch_size, 1, output_dim), dtype=np.float32)
    predicted_audio_seq = []

    for _ in range(max_length):  # Generate step by step
        output, h, c = decoder_model.predict([target_seq] + list(states), verbose=0)
        predicted_audio_seq.append(output[:, 0, :])  # Store predictions for batch

        # Update for next step
        target_seq = np.reshape(output[:, 0, :], (batch_size, 1, output_dim))  # Use predicted output as next input
        states = [h, c]

    return np.array(predicted_audio_seq).transpose(1, 0, 2)  # Shape: (batch, time, features)


# Testing
for i in range(100):  # Predict on first 5 test samples
    predicted_audio = predict_audio_batch(video_test[i:i+1])
    actual_audio = audio_test[i, 0, :]  # Ground truth

    # Flatten for similarity comparison
    actual_audio_flat = actual_audio.flatten()
    predicted_audio_flat = predicted_audio.flatten()

    # Compute Cosine Similarity
    cos_sim = cosine_similarity(
        tf.constant([actual_audio_flat]), tf.constant([predicted_audio_flat])
    ).numpy()

    print(f"\n=== Sample {i+1} ===")
    print(f"Predicted Audio: {predicted_audio.round(3)}")
    print(f"Actual Audio:    {actual_audio.round(3)}")
    print(f"Cosine Similarity: {cos_sim:.4f}")





# LSTM Decoder (map audio features for output)
decoder_inputs = keras.Input(shape=(1, output_dim))
decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)

# mapping layer
decoder_dense = keras.layers.Dense(output_dim, activation="linear") #for predicting continous values
decoder_outputs = decoder_dense(decoder_outputs)

def cosine_similarity(y_true, y_pred):
    y_true = K.l2_normalize(y_true, axis=-1)  # Normalize vectors
    y_pred = K.l2_normalize(y_pred, axis=-1)
    return K.mean(K.sum(y_true * y_pred, axis=-1))  # Compute dot product

# Compile the model
model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer="adam", loss="mse", metrics=["mae", cosine_similarity]) # chose mean squared error loss because the values are continous and not one-hot encoded
model.summary()

# Model Training
batch_size = 64
epochs = 50

history = model.fit(
    [video_train, audio_train],
    decoder_target_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=([video_val, audio_val], decoder_target_val),
    verbose=1
)

# ✅ Print Average Metrics
avg_train_loss = np.mean(history.history['loss'])
avg_val_loss = np.mean(history.history['val_loss'])
avg_train_mae = np.mean(history.history['mae'])
avg_val_mae = np.mean(history.history['val_mae'])
avg_train_cosine_sim = np.mean(history.history.get('cosine_similarity', [0]))
avg_val_cosine_sim = np.mean(history.history.get('val_cosine_similarity', [0]))

print("\n===== FINAL AVERAGE METRICS =====")
print(f"Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}")
print(f"Avg Train MAE: {avg_train_mae:.4f}, Avg Val MAE: {avg_val_mae:.4f}")
print(f"Avg Train Cosine Similarity: {avg_train_cosine_sim:.4f}, Avg Val Cosine Similarity: {avg_val_cosine_sim:.4f}")

# Evaluate on test set
test_loss, test_mae, test_cosine_sim = model.evaluate([video_test, audio_test], decoder_target_test)
print(f"\nTest Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}, Test Cosine Similarity: {test_cosine_sim:.4f}")

# Evaluation

# Plot training loss and MAE
plt.figure(figsize=(12, 4))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label="Training Loss")
plt.plot(history.history['val_loss'], label="Validation Loss")
plt.title("Loss over epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# MAE plot
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label="Training MAE")
plt.plot(history.history['val_mae'], label="Validation MAE")
plt.title("Mean Absolute Error over epochs")
plt.xlabel("Epochs")
plt.ylabel("MAE")
plt.legend()

plt.show()

# Save the trained model
model.save("/content/drive/My Drive/symbol_grounding_prototype/models/DST_Seq2Seq_Model.h5")

# To load the model later
# model = keras.models.load_model("/content/drive/My Drive/symbol_grounding_prototype/models/DST_Seq2Seq_Model.h5")

# ✅ Define Separate Encoder Model for Inference
encoder_model = keras.Model(encoder_inputs, encoder_states)  # Extract encoder states

# ✅ Define Separate Decoder Model for Inference
decoder_state_input_h = keras.Input(shape=(latent_dim,))
decoder_state_input_c = keras.Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# Decoder LSTM (same as training)
decoder_lstm_outputs, state_h_dec, state_c_dec = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]

# Decoder Dense Layer (same as training)
decoder_outputs = decoder_dense(decoder_lstm_outputs)

# ✅ Define Decoder Model
decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs,  # Inputs: decoder input + states
    [decoder_outputs] + decoder_states         # Outputs: decoded features + states
)


# ✅ Prediction Function
def predict_audio(video_sequence):
    """
    Predicts the corresponding audio latent feature for a given video latent feature.
    """
    # Get encoder states from input video features
    states_value = encoder_model.predict(video_sequence)

    # Prepare decoder input (initialize with zeros)
    target_seq = np.zeros((1, 1, output_dim))

    # Run decoder once (single time step)
    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

    return output_tokens[0, 0, :]  # Return the predicted audio feature vector


# ✅ Example Prediction
predicted_audio = predict_audio(video_test[:1])  # Predict for one sample
print("Predicted Audio Latent Features:", predicted_audio)